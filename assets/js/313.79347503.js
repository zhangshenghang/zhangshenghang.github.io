(window.webpackJsonp=window.webpackJsonp||[]).push([[313],{677:function(a,e,s){"use strict";s.r(e);var t=s(8),n=Object(t.a)({},(function(){var a=this,e=a._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"spark-rdd分区2g限制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#spark-rdd分区2g限制"}},[a._v("#")]),a._v(" Spark RDD分区2G限制")]),a._v(" "),e("p",[a._v("[toc]")]),a._v(" "),e("h2",{attrs:{id:"问题现象"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#问题现象"}},[a._v("#")]),a._v(" 问题现象")]),a._v(" "),e("p",[a._v("遇到这个问题时，spark日志会报如下的日志")]),a._v(" "),e("p",[a._v("片段1：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("15/04/16 14:13:03 WARN scheduler.TaskSetManager: Lost task 19.0 in stage 6.0 (TID 120, 10.215.149.47): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\nat sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:828)\nat org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:123)\nat org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:132)\nat org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:517)\nat org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:432)\nat org.apache.spark.storage.BlockManager.get(BlockManager.scala:618)\nat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:146)\nat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br")])]),e("p",[a._v("片段2：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("15/04/16 14:19:45 INFO scheduler.TaskSetManager: Starting task 20.2 in stage 6.0 (TID 146, 10.196.151.213, PROCESS_LOCAL, 1666 bytes)\n\n15/04/16 14:19:45 INFO scheduler.TaskSetManager: Lost task 20.2 in stage 6.0 (TID 146) on executor 10.196.151.213: java.lang.IllegalArgumentException (Size exceeds Integer.MAX_VALUE) [duplicate 1]\n\n15/04/16 14:19:45 INFO scheduler.TaskSetManager: Starting task 20.3 in stage 6.0 (TID 147, 10.196.151.213, PROCESS_LOCAL, 1666 bytes)\n\n15/04/16 14:19:45 INFO scheduler.TaskSetManager: Lost task 20.3 in stage 6.0 (TID 147) on executor 10.196.151.213: java.lang.IllegalArgumentException (Size exceeds Integer.MAX_VALUE) [duplicate 2]\n\n15/04/16 14:19:45 ERROR scheduler.TaskSetManager: Task 20 in stage 6.0 failed 4 times; aborting job\n\n15/04/16 14:19:45 INFO cluster.YarnClusterScheduler: Cancelling stage 6\n\n15/04/16 14:19:45 INFO cluster.YarnClusterScheduler: Stage 6 was cancelled\n\n15/04/16 14:19:45 INFO scheduler.DAGScheduler: Job 6 failed: collectAsMap at DecisionTree.scala:653, took 239.760845 s\n\n15/04/16 14:19:45 ERROR yarn.ApplicationMaster: User class threw exception: Job aborted due to stage failure: Task 20 in stage 6.0 failed 4 times, most recent failure: Lost task 20.3 in stage 6.0 (TID 147, 10.196.151.213): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n\nat sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:828)\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br")])]),e("p",[e("strong",[a._v("异常就是某个partition的数据量超过了Integer.MAX_VALUE（2147483647 = 2GB）")])]),a._v(" "),e("h2",{attrs:{id:"解决方法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#解决方法"}},[a._v("#")]),a._v(" 解决方法")]),a._v(" "),e("p",[a._v("手动设置RDD的分区数量。当前使用的Spark默认RDD分区是18个，后来手动设置为1000个，上面这个问题就迎刃而解了。可以在RDD加载后，使用RDD.repartition(numPart:Int)函数重新设置分区数量。")]),a._v(" "),e("h2",{attrs:{id:"为什么2g限制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#为什么2g限制"}},[a._v("#")]),a._v(" 为什么2G限制")]),a._v(" "),e("p",[a._v("目前spark社区对这个限制有很多讨（tu）论（cao），spark官方团队已经注意到了这个问题，但是直到1.2版本，这个问题还是没有解决。因为牵涉到整个RDD的实现框架，所以改进成本相当大!")]),a._v(" "),e("p",[a._v("下面是一些相关的资料，有兴趣的读者可以进一步的阅读:")]),a._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://issues.apache.org/jira/browse/SPARK-1476",target:"_blank",rel:"noopener noreferrer"}},[a._v("2GB limit in spark for blocks"),e("OutboundLink")],1)]),a._v(" "),e("li",[e("a",{attrs:{href:"https://issues.apache.org/jira/browse/SPARK-6190",target:"_blank",rel:"noopener noreferrer"}},[a._v("create LargeByteBuffer abstraction for eliminating 2GB limit on blocks"),e("OutboundLink")],1)]),a._v(" "),e("li",[e("a",{attrs:{href:"http://stackoverflow.com/questions/29689719/why-does-spark-rdd-partition-has-2gb-limit-for-hdfs/29690454",target:"_blank",rel:"noopener noreferrer"}},[a._v("Why does Spark RDD partition has 2GB limit for HDFS"),e("OutboundLink")],1)]),a._v(" "),e("li",[e("a",{attrs:{href:"http://hg.openjdk.java.net/jdk8u/jdk8u-dev/jdk/file/4a1e42601d61/src/share/classes/sun/nio/ch/FileChannelImpl.java",target:"_blank",rel:"noopener noreferrer"}},[a._v("抛异常的java代码:FileChannelImpl.java"),e("OutboundLink")],1)])]),a._v(" "),e("h2",{attrs:{id:"个人思-yu-考-jian"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#个人思-yu-考-jian"}},[a._v("#")]),a._v(" 个人思（yu）考（jian）")]),a._v(" "),e("p",[a._v("这个限制有一定合理性。因为RDD中partition的操作是并发执行的，如果partition量过少，导致并发数过少，会限制计算效率。所以，基于这个限制，spark应用程序开发者会主动扩大partition数量，也就是加大并发量，最终提高计算性能。")]),a._v(" "),e("p",[a._v("转载自：https://www.cnblogs.com/bourneli/p/4456109.html")])])}),[],!1,null,null,null);e.default=n.exports}}]);